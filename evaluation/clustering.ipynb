{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "from data_loaders import *\n",
    "from missing_process.block_rules import *\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics import (\n",
    "    homogeneity_score,\n",
    "    completeness_score,\n",
    "    v_measure_score,\n",
    ")\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    mutual_info_score,\n",
    "    normalized_mutual_info_score,\n",
    "    adjusted_mutual_info_score,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster_datalist = [\n",
    "#     \"banknote\",\n",
    "#         \"concrete_compression\",\n",
    "#             \"climate_model_crashes\",\n",
    "#             \"connectionist_bench_sonar\",\"qsar_biodegradation\",\n",
    "#             \"yeast\"\n",
    "#             ]\n",
    "\n",
    "real_datalist = [\n",
    "    \"banknote\",\n",
    "        \"concrete_compression\",\n",
    "            \"wine_quality_white\",\"wine_quality_red\",\n",
    "            \"california\",\"climate_model_crashes\",\n",
    "            \"connectionist_bench_sonar\",\"qsar_biodegradation\",\n",
    "            \"yeast\",\"yacht_hydrodynamics\"\n",
    "            ]\n",
    "\n",
    "\n",
    "# except_list = [\"banknote\"\n",
    "# \"climate_model_crashes\"\n",
    "# \"connectionist_bench_sonar\"]\n",
    "\n",
    "real_datalist = [\n",
    "        \"concrete_compression\",\n",
    "            \"wine_quality_white\",\"wine_quality_red\",\n",
    "            \"california\",\n",
    "\"qsar_biodegradation\",\n",
    "            \"yeast\",\"yacht_hydrodynamics\"\n",
    "            ]\n",
    "\n",
    "missingtypelist = [\n",
    "                    \"quantile\",\n",
    "                   \"diffuse\",\n",
    "                   \"logistic\"\n",
    "                   \"mcar\",\"mar\"\n",
    "                   ]\n",
    "\n",
    "seed = 1\n",
    "nfold = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_impute_data(missingtype,model_name,rule_name,dataname,fold,seed = 1):\n",
    "\n",
    "    if model_name == \"tabcsdi\" and missingtype in [\"mcar\",\"mar\"]:\n",
    "        train_impute = np.load(f'impute_hpc/{missingtype}/{dataname}/{model_name}/{rule_name}_seed-{seed}_{fold}_train.npy')\n",
    "        test_impute = np.load(f'impute_hpc/{missingtype}/{dataname}/{model_name}/{rule_name}_seed-{seed}_{fold}_test.npy')\n",
    "        return train_impute,test_impute\n",
    "\n",
    "    train_impute = np.load(f'impute/{missingtype}/{dataname}/{model_name}/{rule_name}_seed-{seed}_{fold}_train.npy')\n",
    "    test_impute = np.load(f'impute/{missingtype}/{dataname}/{model_name}/{rule_name}_seed-{seed}_{fold}_test.npy')\n",
    "    return train_impute,test_impute\n",
    "\n",
    "def load_train_test(index_file,norm_values,observed_masks):  \n",
    "\n",
    "    train_index = index_file[\"train_index\"]\n",
    "    test_index = index_file[\"test_index\"]\n",
    "\n",
    "    train_values = norm_values[train_index,:]\n",
    "\n",
    "    train_masks = observed_masks[train_index,:]\n",
    "\n",
    "    test_values = norm_values[test_index,:]\n",
    "\n",
    "    test_masks = observed_masks[test_index,:]\n",
    "\n",
    "\n",
    "    return train_values,train_masks,test_values,test_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fillin_imputed_data(imputed,mask,original):\n",
    "    filled_data = np.where(mask == 1, original, imputed)\n",
    "    return filled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:00<00:00, 17.54it/s]\n",
      "100%|██████████| 9/9 [00:03<00:00,  2.87it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 12.15it/s]\n",
      " 33%|███▎      | 3/9 [00:09<00:19,  3.19s/it]\n",
      " 43%|████▎     | 3/7 [00:13<00:18,  4.67s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 51\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# print(impute_test)\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# print(test_values)\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# print(test_masks)\u001b[39;00m\n\u001b[0;32m     50\u001b[0m clu_test \u001b[38;5;241m=\u001b[39m DBSCAN()\u001b[38;5;241m.\u001b[39mfit(test_values)\u001b[38;5;241m.\u001b[39mlabels_\n\u001b[1;32m---> 51\u001b[0m clu_test_imp \u001b[38;5;241m=\u001b[39m \u001b[43mDBSCAN\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimpute_test\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mlabels_\n\u001b[0;32m     54\u001b[0m V \u001b[38;5;241m=\u001b[39m v_measure_score(clu_test, clu_test_imp)\n\u001b[0;32m     55\u001b[0m AMI \u001b[38;5;241m=\u001b[39m adjusted_mutual_info_score(clu_test, clu_test_imp)\n",
      "File \u001b[1;32md:\\miniconda3\\envs\\py3.10\\lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\miniconda3\\envs\\py3.10\\lib\\site-packages\\sklearn\\cluster\\_dbscan.py:402\u001b[0m, in \u001b[0;36mDBSCAN.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    400\u001b[0m neighbors_model\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[0;32m    401\u001b[0m \u001b[38;5;66;03m# This has worst case O(n^2) memory complexity\u001b[39;00m\n\u001b[1;32m--> 402\u001b[0m neighborhoods \u001b[38;5;241m=\u001b[39m \u001b[43mneighbors_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mradius_neighbors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_distance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    404\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    405\u001b[0m     n_neighbors \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;28mlen\u001b[39m(neighbors) \u001b[38;5;28;01mfor\u001b[39;00m neighbors \u001b[38;5;129;01min\u001b[39;00m neighborhoods])\n",
      "File \u001b[1;32md:\\miniconda3\\envs\\py3.10\\lib\\site-packages\\sklearn\\neighbors\\_base.py:1235\u001b[0m, in \u001b[0;36mRadiusNeighborsMixin.radius_neighbors\u001b[1;34m(self, X, radius, return_distance, sort_results)\u001b[0m\n\u001b[0;32m   1233\u001b[0m n_jobs \u001b[38;5;241m=\u001b[39m effective_n_jobs(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs)\n\u001b[0;32m   1234\u001b[0m delayed_query \u001b[38;5;241m=\u001b[39m delayed(_tree_query_radius_parallel_helper)\n\u001b[1;32m-> 1235\u001b[0m chunked_results \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mthreads\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1236\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed_query\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1237\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tree\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43ms\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mradius\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_distance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msort_results\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort_results\u001b[49m\n\u001b[0;32m   1238\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1239\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgen_even_slices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1240\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1241\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_distance:\n\u001b[0;32m   1242\u001b[0m     neigh_ind, neigh_dist \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mchunked_results))\n",
      "File \u001b[1;32md:\\miniconda3\\envs\\py3.10\\lib\\site-packages\\sklearn\\utils\\parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     60\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     61\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     64\u001b[0m )\n\u001b[1;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\miniconda3\\envs\\py3.10\\lib\\site-packages\\joblib\\parallel.py:1085\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1076\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1077\u001b[0m     \u001b[38;5;66;03m# Only set self._iterating to True if at least a batch\u001b[39;00m\n\u001b[0;32m   1078\u001b[0m     \u001b[38;5;66;03m# was dispatched. In particular this covers the edge\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1082\u001b[0m     \u001b[38;5;66;03m# was very quick and its callback already dispatched all the\u001b[39;00m\n\u001b[0;32m   1083\u001b[0m     \u001b[38;5;66;03m# remaining jobs.\u001b[39;00m\n\u001b[0;32m   1084\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m-> 1085\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_one_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1086\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1088\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n",
      "File \u001b[1;32md:\\miniconda3\\envs\\py3.10\\lib\\site-packages\\joblib\\parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    899\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    900\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 901\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    902\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32md:\\miniconda3\\envs\\py3.10\\lib\\site-packages\\joblib\\parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    817\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    818\u001b[0m     job_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs)\n\u001b[1;32m--> 819\u001b[0m     job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    820\u001b[0m     \u001b[38;5;66;03m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    821\u001b[0m     \u001b[38;5;66;03m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    822\u001b[0m     \u001b[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    823\u001b[0m     \u001b[38;5;66;03m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    824\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs\u001b[38;5;241m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[1;32md:\\miniconda3\\envs\\py3.10\\lib\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_async\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mImmediateResult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback:\n\u001b[0;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[1;32md:\\miniconda3\\envs\\py3.10\\lib\\site-packages\\joblib\\_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[0;32m    595\u001b[0m     \u001b[38;5;66;03m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    596\u001b[0m     \u001b[38;5;66;03m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> 597\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\miniconda3\\envs\\py3.10\\lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[1;32md:\\miniconda3\\envs\\py3.10\\lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[1;32md:\\miniconda3\\envs\\py3.10\\lib\\site-packages\\sklearn\\utils\\parallel.py:127\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    125\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\miniconda3\\envs\\py3.10\\lib\\site-packages\\sklearn\\neighbors\\_base.py:1013\u001b[0m, in \u001b[0;36m_tree_query_radius_parallel_helper\u001b[1;34m(tree, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1007\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_tree_query_radius_parallel_helper\u001b[39m(tree, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   1008\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Helper for the Parallel calls in RadiusNeighborsMixin.radius_neighbors.\u001b[39;00m\n\u001b[0;32m   1009\u001b[0m \n\u001b[0;32m   1010\u001b[0m \u001b[38;5;124;03m    The Cython method tree.query_radius is not directly picklable by\u001b[39;00m\n\u001b[0;32m   1011\u001b[0m \u001b[38;5;124;03m    cloudpickle under PyPy.\u001b[39;00m\n\u001b[0;32m   1012\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1013\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tree\u001b[38;5;241m.\u001b[39mquery_radius(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Single Model Name\n",
    "\n",
    "datalist = real_datalist\n",
    "model_name = \"tabcsdi\"\n",
    "missingtypelist = [\"logistic\"]\n",
    "for missingtype in missingtypelist:\n",
    "    if missingtype == \"logistic\":\n",
    "        missing_rule = load_json_file(\"missing_rate.json\")\n",
    "    elif missingtype == \"diffuse\":\n",
    "        missing_rule = load_json_file(\"diffuse_ratio.json\")\n",
    "    elif missingtype == \"quantile\":\n",
    "        missing_rule = load_json_file(\"quantile_full.json\")\n",
    "    elif missingtype == \"mcar\" or missingtype == \"mar\":\n",
    "        missing_rule = load_json_file(\"mcar.json\")\n",
    "    d_v = {}\n",
    "    d_AMI = {}\n",
    "\n",
    "    # load data and its mask\n",
    "    for dataname in tqdm(datalist):\n",
    "        \n",
    "        directory_path = f\"datasets/{dataname}\"\n",
    "        data = dataset_loader(dataname)    \n",
    "        norm_values = np.load(f'{directory_path}/{dataname}_norm.npy')\n",
    "\n",
    "        # clustering = DBSCAN().fit(norm_values)\n",
    "        # #print(clustering.labels_)\n",
    "        # ncluster = len(set(clustering.labels_))\n",
    "        # if ncluster == 1:\n",
    "        #     print(dataname)\n",
    "        d_v[dataname] = []\n",
    "        d_AMI[dataname] = []\n",
    "\n",
    "        for rule_name in tqdm(missing_rule):\n",
    "            \n",
    "            observed_masks = np.load(f'{directory_path}/{missingtype}/{rule_name}.npy')\n",
    "            f = open(f'{directory_path}/split_index_cv_seed-{seed}_nfold-{nfold}.json')\n",
    "            index_file = json.load(f)\n",
    "\n",
    "            v_list = []\n",
    "            AMI_list = []\n",
    "            for fold in index_file:\n",
    "                index = index_file[fold]\n",
    "                train_values,train_masks,test_values,test_masks = load_train_test(index,norm_values,observed_masks)\n",
    "                impute_train,impute_test  = load_impute_data(missingtype,model_name,rule_name,dataname,fold)\n",
    "\n",
    "                impute_test = fillin_imputed_data(impute_test,test_masks,test_values)\n",
    "                # print(impute_test)\n",
    "                # print(test_values)\n",
    "                # print(test_masks)\n",
    "                clu_test = DBSCAN().fit(test_values).labels_\n",
    "                clu_test_imp = DBSCAN().fit(impute_test).labels_\n",
    "\n",
    "                \n",
    "                V = v_measure_score(clu_test, clu_test_imp)\n",
    "                AMI = adjusted_mutual_info_score(clu_test, clu_test_imp)\n",
    "\n",
    "                v_list.append(V)\n",
    "                AMI_list.append(AMI)\n",
    "            \n",
    "\n",
    "            d_v[dataname].append(np.mean(v_list))\n",
    "            d_AMI[dataname] .append(np.mean(AMI_list))\n",
    "\n",
    "    df_v = pd.DataFrame(d_v,index=[rule_name for rule_name in missing_rule])\n",
    "    df_AMI = pd.DataFrame(d_AMI,index=[rule_name for rule_name in missing_rule])\n",
    "\n",
    "    path = f\"clustering/{missingtype}/\"\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "    df_v.to_csv(f\"{path}/{model_name}_V.csv\")\n",
    "\n",
    "    df_AMI.to_csv(f'{path}/{model_name}_AMI.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fillin_imputed_data(imputed, mask, original):\n",
    "    \"\"\"\n",
    "    Fill in missing values in the imputed dataset using the original dataset and the mask.\n",
    "    Replace NaNs in the imputed dataset with 1 when the mask indicates missing values.\n",
    "\n",
    "    Parameters:\n",
    "        imputed (numpy.ndarray): Imputed dataset with missing values.\n",
    "        mask (numpy.ndarray): Mask indicating missing values (NaNs).\n",
    "        original (numpy.ndarray): Original dataset with complete values.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: New dataset with missing values filled in.\n",
    "    \"\"\"\n",
    "    filled_data = np.where(np.isnan(mask), 1, imputed)\n",
    "    return filled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "datalist = real_datalist\n",
    "\n",
    "\n",
    "def run_cluster(rule_name,model_name_list,missingtype,datalist):\n",
    "\n",
    "    d_v = {}\n",
    "    d_AMI= {}\n",
    "\n",
    "    for dataname in tqdm(datalist):\n",
    "        \n",
    "        directory_path = f\"datasets/{dataname}\"\n",
    "        data = dataset_loader(dataname)    \n",
    "        norm_values = np.load(f'{directory_path}/{dataname}_norm.npy')\n",
    "        \n",
    "        observed_masks = np.load(f'{directory_path}/{missingtype}/{rule_name}.npy')\n",
    "        f = open(f'{directory_path}/split_index_cv_seed-{seed}_nfold-{nfold}.json')\n",
    "        index_file = json.load(f)\n",
    "\n",
    "        d_v[dataname] = {}\n",
    "        d_AMI[dataname] = {}\n",
    "\n",
    "        for model_name in model_name_list:\n",
    "            d_v[dataname][model_name]=0\n",
    "            d_AMI[dataname][model_name]=0\n",
    "            # load data and its mask\n",
    "\n",
    "            v_list = []\n",
    "            AMI_list = []\n",
    "            for fold in index_file:\n",
    "                index = index_file[fold]\n",
    "                train_values,train_masks,test_values,test_masks = load_train_test(index,norm_values,observed_masks)\n",
    "                impute_train,impute_test  = load_impute_data(missingtype,model_name,rule_name,dataname,fold)\n",
    "\n",
    "                impute_test = fillin_imputed_data(impute_test,test_masks,test_values)\n",
    "                impute_test = np.nan_to_num(impute_test, nan=0)\n",
    "\n",
    "                clu_test = DBSCAN().fit(test_values).labels_\n",
    "                clu_test_imp = DBSCAN().fit(impute_test).labels_\n",
    "\n",
    "                \n",
    "                V = v_measure_score(clu_test, clu_test_imp)\n",
    "                AMI = adjusted_mutual_info_score(clu_test, clu_test_imp)\n",
    "\n",
    "                v_list.append(V)\n",
    "                AMI_list.append(AMI)\n",
    "            \n",
    "\n",
    "            d_v[dataname][model_name]=np.mean(v_list)\n",
    "            d_AMI[dataname][model_name]=np.mean(AMI_list)\n",
    "\n",
    "\n",
    "    df_v = pd.DataFrame(d_v).T\n",
    "    df_AMI = pd.DataFrame(d_AMI).T\n",
    "    new_col_names = [\"RD\", \"ZR\", \"Mean\", \"KNN\", \"MF\", \"Mice\", \"MisF\", \"XGB\", \"OT\", \"HI\", \"GAIN\", \"Mi\", \"NMi\", \"CSDI\"]\n",
    "    new_row_names = [\"Bank\",\"Cali\",\"Climate\",\"Concre\",\"Qsar\",  \"Red\",  \"Sonar\", \"White\", \"Yacht\",\"Yeast\"]\n",
    "\n",
    "    path = f\"clustering/{missingtype}/\"\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "    df_v = df_v.rename(index=dict(zip(df_v.index, new_row_names)), columns=dict(zip(df_v.columns, new_col_names)))\n",
    "    df_AMI = df_AMI.rename(index=dict(zip(df_AMI.index, new_row_names)), columns=dict(zip(df_AMI.columns, new_col_names)))\n",
    "\n",
    "    df_v.to_csv(f\"{path}/{rule_name}_V.csv\")\n",
    "\n",
    "    df_AMI.to_csv(f'{path}/{rule_name}_AMI.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:49<00:00,  4.92s/it]\n",
      "100%|██████████| 10/10 [00:49<00:00,  4.91s/it]\n",
      "100%|██████████| 10/10 [00:50<00:00,  5.01s/it]\n",
      "100%|██████████| 10/10 [00:49<00:00,  4.95s/it]\n",
      "100%|██████████| 10/10 [00:50<00:00,  5.00s/it]\n",
      "100%|██████████| 10/10 [00:49<00:00,  4.97s/it]\n"
     ]
    }
   ],
   "source": [
    "model_name_list = [\"random\", \"zero\", \"mean\", \"knn\", \"mf\", \"mice\", \"missforest\", \"XGB\", \"ot\", \"hyper\", \"gain\", \"miwae\", \"notmiwae\", \"tabcsdi\"]\n",
    "\n",
    "# real_datalist = [\n",
    "#         \"concrete_compression\",\n",
    "#             \"wine_quality_white\",\"wine_quality_red\",\n",
    "#             \"california\",\n",
    "# \"qsar_biodegradation\",\n",
    "#             \"yeast\",\"yacht_hydrodynamics\"\n",
    "#             ]\n",
    "\n",
    "\n",
    "real_datalist = [\"banknote\",\n",
    "            \"california\",\"climate_model_crashes\",\"concrete_compression\",\n",
    "           \"qsar_biodegradation\",\"wine_quality_red\", \"connectionist_bench_sonar\",\"wine_quality_white\",\n",
    "            \"yacht_hydrodynamics\",\"yeast\"\n",
    "            ]\n",
    "\n",
    "for missingtype in [\"diffuse\",\"logistic\"]:\n",
    "    for rule_name in [\"0.3\",\"0.5\",\"0.7\"]:\n",
    "        run_cluster(rule_name,model_name_list,missingtype,real_datalist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:37<00:00,  3.78s/it]\n",
      "100%|██████████| 10/10 [00:36<00:00,  3.60s/it]\n",
      "100%|██████████| 10/10 [00:36<00:00,  3.65s/it]\n",
      "100%|██████████| 10/10 [00:42<00:00,  4.25s/it]\n",
      "100%|██████████| 10/10 [00:42<00:00,  4.25s/it]\n",
      "100%|██████████| 10/10 [00:42<00:00,  4.21s/it]\n"
     ]
    }
   ],
   "source": [
    "for missingtype in [\"mar\",\"mcar\"]:\n",
    "    for rule_name in [\"0.3\",\"0.5\",\"0.7\"]:\n",
    "        run_cluster(rule_name,model_name_list,missingtype,real_datalist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:52<00:00,  5.27s/it]\n",
      "100%|██████████| 10/10 [00:51<00:00,  5.16s/it]\n",
      "100%|██████████| 10/10 [00:52<00:00,  5.22s/it]\n"
     ]
    }
   ],
   "source": [
    "missingtype = \"quantile\"\n",
    "for rule_name in [\"Q1_Q4_0.5\",\"Q2_Q3_0.5\",\"Q2_Q4_0.5\"]:\n",
    "    run_cluster(rule_name,model_name_list,missingtype,real_datalist)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
